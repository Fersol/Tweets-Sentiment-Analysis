{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор данных содержит равное количество позитивных и негативных твитов, общее количество которых составляет ~200 000. Мы случайным образом отбираем позитивные и негативные твиты, чтобы разделить исходный набор данных на два набора для обучения и тестирования в соотношении 80/20. Затем обучаем модель Word2Vec на обучающем наборе данных.\n",
    "\n",
    "Чтобы предотвратить утечку данных из тестового набора, мы не обучаем Word2Vec на тестовом наборе, пока наш классификатор не обучится на обучающем наборе. Чтобы создать входные данные для классификатора, используем усредненный вектор, полученный из всех векторов слов в твите. Для реализации машинного обучения будем использовать библиотеку scikit-learn.\n",
    "\n",
    "Вначале импортируем данные и обучаем модель Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksim/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "with open('extra_data/prepared_tweets.txt', 'r') as infile:\n",
    "    tweets = infile.readlines()\n",
    "    \n",
    "with open('extra_data/targets.txt', 'r') as infile:\n",
    "    y = infile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tweets, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train num: 181467\n",
      "y_train num: 181467\n",
      "x_test num: 45367\n",
      "x_test num: 45367\n"
     ]
    }
   ],
   "source": [
    "print \"x_train num:\", len(x_train)\n",
    "print \"y_train num:\", len(y_train)\n",
    "print \"x_test num:\", len(x_test)\n",
    "print \"x_test num:\", len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanText(corpus):\n",
    "    corpus = [z.lower().replace('\\n','').split() for z in corpus]\n",
    "    return corpus\n",
    "\n",
    "x_train = cleanText(x_train)\n",
    "x_test = cleanText(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = [int(y.strip()) for y in y_train]\n",
    "y_test = [int(y.strip()) for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# список параметров, которые можно менять по вашему желанию\n",
    "num_features = 512  # итоговая размерность вектора каждого слова\n",
    "min_word_count = 5  # минимальная частотность слова, чтобы оно попало в модель\n",
    "num_workers = 8     # количество ядер вашего процессора, чтоб запустить обучение в несколько потоков\n",
    "context = 10        # размер окна \n",
    "downsampling = 1e-3 # внутренняя метрика модели\n",
    "\n",
    "w2v = Word2Vec(workers=num_workers, size=num_features,\n",
    "                 min_count=min_word_count, window=context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build vocab\n",
    "w2v.build_vocab(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7917082"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model over train_reviews (this may take several minutes)\n",
    "w2v.train(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build word vector for training set by using the average value of all word vectors in the tweet, then scale\n",
    "def buildWordVector(text, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Масштабирование – это часть процесса стандартизации, в рамках которого мы приводим наш набор данных к гауссовому распределению со средним значением, равным нулю.\n",
    "\n",
    "Как следствие, значения, превышающие среднее, будут положительными, а те, что меньше него, – отрицательными. Для эффективной работы многих моделей машинного обучения требуются масштабированные наборы данных, особенно для моделей с большим количеством признаков, таких как текстовые классификаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs = np.concatenate([buildWordVector(z, num_features) for z in x_train])\n",
    "train_vecs = scale(train_vecs)\n",
    "\n",
    "#Train word2vec on test tweets\n",
    "w2v.train(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы должны создать векторы тестового набора и масштабировать их для оценки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build test tweet vectors then scale\n",
    "test_vecs = np.concatenate([buildWordVector(z, num_features) for z in x_test])\n",
    "test_vecs = scale(test_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы проверим наш классификатор, рассчитав точность прогнозирования на тестовых данных, а также проанализируем ROC-кривую. ROC-кривая характеризует соотношение истинноположительных и ложноположительных результатов классификации при варьировании параметра модели.\n",
    "\n",
    "В нашем случае мы регулируем пороговую вероятность классификации твита, как позитивного или негативного. В общем случае, чем больше площадь под кривой (AUC), тем лучше наша модель максимизирует количество истинноположительных и минимизирует количество ложноположительных результатов. \n",
    "\n",
    "Для начала мы обучаем наш классификатор, используя в данном случае стохастический градиентный спуск (stochastic gradient descent) для логистической регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use classification algorithm (i.e. Stochastic Logistic Regression) on training set, then assess model performance on test set\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "lr = SGDClassifier(loss='log', penalty='l1')\n",
    "lr.fit(train_vecs, y_train)\n",
    "\n",
    "print 'Test Accuracy: %.2f'%lr.score(test_vecs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"algo\", LogisticRegression())])\n",
    "\n",
    "param_dict = {\n",
    "              'algo__C': [0.01, 0.1, 1, 10, 100],\n",
    "              'algo__penalty': ['l2', 'l1']}\n",
    "\n",
    "estimator = GridSearchCV(pipe, param_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred_probas = lr.predict_proba(test_vecs)[:,1]\n",
    "\n",
    "fpr,tpr,_ = roc_curve(y_test, pred_probas)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
